{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install and Import Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-4.2.0-py3-none-any.whl (506 kB)\n",
            "     ------------------------------------ 506.3/506.3 KB 705.7 kB/s eta 0:00:00\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp310-cp310-win_amd64.whl (18.2 MB)\n",
            "     -------------------------------------- 18.2/18.2 MB 486.2 kB/s eta 0:00:00\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n",
            "     ------------------------------------ 486.6/486.6 KB 423.4 kB/s eta 0:00:00\n",
            "Requirement already satisfied: pandas in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.1)\n",
            "Collecting dill<0.4.1,>=0.3.0\n",
            "  Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "     ------------------------------------ 119.7/119.7 KB 701.5 kB/s eta 0:00:00\n",
            "Collecting filelock\n",
            "  Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
            "Collecting fsspec[http]<=2025.9.0,>=2023.1.0\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "     ------------------------------------ 199.3/199.3 KB 327.1 kB/s eta 0:00:00\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading pyyaml-6.0.3-cp310-cp310-win_amd64.whl (158 kB)\n",
            "     ------------------------------------ 158.6/158.6 KB 256.6 kB/s eta 0:00:00\n",
            "Collecting tqdm>=4.66.3\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "     -------------------------------------- 78.5/78.5 KB 291.9 kB/s eta 0:00:00\n",
            "Collecting huggingface-hub<2.0,>=0.25.0\n",
            "  Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
            "     ------------------------------------ 564.3/564.3 KB 437.6 kB/s eta 0:00:00\n",
            "Collecting multiprocess<0.70.17\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "     ------------------------------------ 134.8/134.8 KB 379.8 kB/s eta 0:00:00\n",
            "Requirement already satisfied: httpx<1.0.0 in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\marcf\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (24.2)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.6.0-cp310-cp310-win_amd64.whl (31 kB)\n",
            "Collecting pyarrow>=21.0.0\n",
            "  Downloading pyarrow-21.0.0-cp310-cp310-win_amd64.whl (26.2 MB)\n",
            "     -------------------------------------- 26.2/26.2 MB 482.3 kB/s eta 0:00:00\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.32.3)\n",
            "Collecting transformers<5.0.0,>=4.41.0\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "     -------------------------------------- 12.0/12.0 MB 489.8 kB/s eta 0:00:00\n",
            "Collecting torch>=1.11.0\n",
            "  Downloading torch-2.9.0-cp310-cp310-win_amd64.whl (109.3 MB)\n",
            "     ------------------------------------ 109.3/109.3 MB 413.9 kB/s eta 0:00:00\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: Pillow in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: scipy in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.15.2)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\marcf\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\marcf\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.13.0-cp310-cp310-win_amd64.whl (453 kB)\n",
            "     ------------------------------------ 453.0/453.0 KB 240.2 kB/s eta 0:00:00\n",
            "Requirement already satisfied: idna in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
            "Requirement already satisfied: anyio in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: certifi in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1.0.0->datasets) (2024.12.14)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\marcf\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Collecting sympy>=1.13.3\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting networkx>=2.5.1\n",
            "  Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "Requirement already satisfied: colorama in c:\\users\\marcf\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
            "     ---------------------------------------- 2.7/2.7 MB 368.3 kB/s eta 0:00:00\n",
            "Collecting safetensors>=0.4.3\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
            "     ------------------------------------ 320.2/320.2 KB 763.4 kB/s eta 0:00:00\n",
            "Collecting regex!=2019.12.17\n",
            "  Downloading regex-2025.9.18-cp310-cp310-win_amd64.whl (276 kB)\n",
            "     ------------------------------------ 276.1/276.1 KB 549.4 kB/s eta 0:00:00\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Collecting aiohappyeyeballs>=2.5.0\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (24.3.0)\n",
            "Collecting aiosignal>=1.4.0\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Collecting propcache>=0.2.0\n",
            "  Downloading propcache-0.4.1-cp310-cp310-win_amd64.whl (41 kB)\n",
            "     -------------------------------------- 41.6/41.6 KB 402.5 kB/s eta 0:00:00\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.7.0-cp310-cp310-win_amd64.whl (46 kB)\n",
            "     -------------------------------------- 46.0/46.0 KB 163.5 kB/s eta 0:00:00\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.8.0-cp310-cp310-win_amd64.whl (43 kB)\n",
            "     -------------------------------------- 43.8/43.8 KB 357.4 kB/s eta 0:00:00\n",
            "Collecting async-timeout<6.0,>=4.0\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Collecting yarl<2.0,>=1.17.0\n",
            "  Downloading yarl-1.22.0-cp310-cp310-win_amd64.whl (86 kB)\n",
            "     -------------------------------------- 86.9/86.9 KB 288.0 kB/s eta 0:00:00\n",
            "Collecting mpmath<1.4,>=1.1.0\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\marcf\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx<1.0.0->datasets) (1.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\marcf\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Installing collected packages: mpmath, xxhash, tqdm, sympy, safetensors, regex, pyyaml, pyarrow, propcache, networkx, multidict, fsspec, frozenlist, filelock, faiss-cpu, dill, async-timeout, aiohappyeyeballs, yarl, torch, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, sentence-transformers, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.0 aiosignal-1.4.0 async-timeout-5.0.1 datasets-4.2.0 dill-0.4.0 faiss-cpu-1.12.0 filelock-3.20.0 frozenlist-1.8.0 fsspec-2025.9.0 huggingface-hub-0.35.3 mpmath-1.3.0 multidict-6.7.0 multiprocess-0.70.16 networkx-3.4.2 propcache-0.4.1 pyarrow-21.0.0 pyyaml-6.0.3 regex-2025.9.18 safetensors-0.6.2 sentence-transformers-5.1.1 sympy-1.14.0 tokenizers-0.22.1 torch-2.9.0 tqdm-4.67.1 transformers-4.57.1 xxhash-3.6.0 yarl-1.22.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script tqdm.exe is installed in 'c:\\Users\\marcf\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script isympy.exe is installed in 'c:\\Users\\marcf\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'c:\\Users\\marcf\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The scripts hf.exe, huggingface-cli.exe and tiny-agents.exe are installed in 'c:\\Users\\marcf\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The scripts transformers-cli.exe and transformers.exe are installed in 'c:\\Users\\marcf\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script datasets-cli.exe is installed in 'c:\\Users\\marcf\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "WARNING: You are using pip version 22.0.4; however, version 25.2 is available.\n",
            "You should consider upgrading via the 'c:\\Users\\marcf\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install datasets faiss-cpu sentence-transformers pandas numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\marcf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import List, Tuple\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 100%|██████████| 853/853 [00:00<00:00, 71046.14 examples/s]\n",
            "Generating test split: 100%|██████████| 212/212 [00:00<00:00, 8287.05 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully!\n",
            "Available splits: ['train', 'test']\n"
          ]
        }
      ],
      "source": [
        "# Load the symptom-to-diagnosis dataset\n",
        "print(\"Loading dataset...\")\n",
        "ds = load_dataset(\"gretelai/symptom_to_diagnosis\")\n",
        "print(f\"Dataset loaded successfully!\")\n",
        "print(f\"Available splits: {list(ds.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of examples: 853\n",
            "\n",
            "First example:\n",
            "Input (symptoms): I've been having a lot of pain in my neck and back. I've also been having trouble with my balance and coordination. I've been coughing a lot and my limbs feel weak.\n",
            "Output (diagnosis): cervical spondylosis\n"
          ]
        }
      ],
      "source": [
        "# Explore the dataset structure\n",
        "train_data = ds['train']\n",
        "print(f\"Number of examples: {len(train_data)}\")\n",
        "print(f\"\\nFirst example:\")\n",
        "print(f\"Input (symptoms): {train_data[0]['input_text']}\")\n",
        "print(f\"Output (diagnosis): {train_data[0]['output_text']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (853, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>output_text</th>\n",
              "      <th>input_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cervical spondylosis</td>\n",
              "      <td>I've been having a lot of pain in my neck and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>impetigo</td>\n",
              "      <td>I have a rash on my face that is getting worse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>urinary tract infection</td>\n",
              "      <td>I have been urinating blood. I sometimes feel ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>arthritis</td>\n",
              "      <td>I have been having trouble with my muscles and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dengue</td>\n",
              "      <td>I have been feeling really sick. My body hurts...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               output_text                                         input_text\n",
              "0     cervical spondylosis  I've been having a lot of pain in my neck and ...\n",
              "1                 impetigo  I have a rash on my face that is getting worse...\n",
              "2  urinary tract infection  I have been urinating blood. I sometimes feel ...\n",
              "3                arthritis  I have been having trouble with my muscles and...\n",
              "4                   dengue  I have been feeling really sick. My body hurts..."
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert to pandas for easier manipulation\n",
        "df = pd.DataFrame(train_data)\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embedding model: all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "Embedding dimension: 384\n"
          ]
        }
      ],
      "source": [
        "# Initialize the embedding model\n",
        "# Using a model optimized for semantic search\n",
        "model_name = 'all-MiniLM-L6-v2'  # Fast and effective model\n",
        "print(f\"Loading embedding model: {model_name}\")\n",
        "embedding_model = SentenceTransformer(model_name)\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Generate Embeddings for the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings for 853 symptom descriptions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 27/27 [00:06<00:00,  4.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Embeddings generated in 6.81 seconds\n",
            "Embeddings shape: (853, 384)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Extract symptom texts\n",
        "symptom_texts = df['input_text'].tolist()\n",
        "print(f\"Generating embeddings for {len(symptom_texts)} symptom descriptions...\")\n",
        "\n",
        "# Generate embeddings with progress tracking\n",
        "start_time = time.time()\n",
        "embeddings = embedding_model.encode(\n",
        "    symptom_texts,\n",
        "    show_progress_bar=True,\n",
        "    batch_size=32,\n",
        "    convert_to_numpy=True\n",
        ")\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"\\nEmbeddings generated in {end_time - start_time:.2f} seconds\")\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build FAISS Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building FAISS index...\n",
            "Index built successfully!\n",
            "Total vectors in index: 853\n"
          ]
        }
      ],
      "source": [
        "# Normalize embeddings for cosine similarity\n",
        "faiss.normalize_L2(embeddings)\n",
        "\n",
        "# Get embedding dimension\n",
        "dimension = embeddings.shape[1]\n",
        "\n",
        "# Create FAISS index (using IndexFlatIP for inner product, equivalent to cosine similarity with normalized vectors)\n",
        "index = faiss.IndexFlatIP(dimension)\n",
        "\n",
        "# Add embeddings to the index\n",
        "print(f\"Building FAISS index...\")\n",
        "index.add(embeddings)\n",
        "print(f\"Index built successfully!\")\n",
        "print(f\"Total vectors in index: {index.ntotal}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create Retrieval Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_top_k(query: str, k: int = 5) -> List[Tuple[float, str, str]]:\n",
        "    \"\"\"\n",
        "    Retrieve top-k most similar symptom-diagnosis pairs for a given query.\n",
        "    \n",
        "    Args:\n",
        "        query: The symptom description query\n",
        "        k: Number of results to return (default: 5)\n",
        "    \n",
        "    Returns:\n",
        "        List of tuples containing (similarity_score, symptoms, diagnosis)\n",
        "    \"\"\"\n",
        "    # Generate embedding for the query\n",
        "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
        "    \n",
        "    # Normalize for cosine similarity\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "    \n",
        "    # Search the index\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    \n",
        "    # Retrieve and format results\n",
        "    results = []\n",
        "    for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
        "        symptom = df.iloc[idx]['input_text']\n",
        "        diagnosis = df.iloc[idx]['output_text']\n",
        "        results.append((distance, symptom, diagnosis))\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def display_results(query: str, results: List[Tuple[float, str, str]]):\n",
        "    \"\"\"\n",
        "    Display retrieval results in a readable format.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(f\"QUERY: {query}\")\n",
        "    print(\"=\"*80)\n",
        "    print()\n",
        "    \n",
        "    for i, (score, symptoms, diagnosis) in enumerate(results, 1):\n",
        "        print(f\"Result {i} (Similarity Score: {score:.4f})\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"Symptoms: {symptoms}\")\n",
        "        print(f\"Diagnosis: {diagnosis}\")\n",
        "        print()\n",
        "    print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Example Query: Retrieve Top 5 Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Searching for top 5 matches...\n",
            "\n",
            "================================================================================\n",
            "QUERY: I have been experiencing severe headaches, high fever, and muscle aches for the past 3 days.\n",
            "================================================================================\n",
            "\n",
            "Result 1 (Similarity Score: 0.8209)\n",
            "--------------------------------------------------------------------------------\n",
            "Symptoms: I have a high fever, chills, nausea, and a headache. I also have muscle aches and a lot of sweating.\n",
            "Diagnosis: malaria\n",
            "\n",
            "Result 2 (Similarity Score: 0.8097)\n",
            "--------------------------------------------------------------------------------\n",
            "Symptoms: I'm experiencing a high fever, chills, nausea, and severe itching. I also have a headache and have been sweating a lot. I've also been experiencing muscle aches.\n",
            "Diagnosis: malaria\n",
            "\n",
            "Result 3 (Similarity Score: 0.7993)\n",
            "--------------------------------------------------------------------------------\n",
            "Symptoms: I've been having a really high fever, chills, and nausea. I've also been sweating a lot and my muscles hurt. I feel really queasy and have a headache.\n",
            "Diagnosis: malaria\n",
            "\n",
            "Result 4 (Similarity Score: 0.7953)\n",
            "--------------------------------------------------------------------------------\n",
            "Symptoms: I have been experiencing severe joint pain and headaches for the past few days. I also have a mild fever and chills. \n",
            "Diagnosis: dengue\n",
            "\n",
            "Result 5 (Similarity Score: 0.7904)\n",
            "--------------------------------------------------------------------------------\n",
            "Symptoms: I've been experiencing a high fever, vomiting, chills, and severe itching. I also have a headache and am sweating profusely. My discomfort has also been accompanied by nausea and muscle pain.\n",
            "Diagnosis: malaria\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Example query with headache and fever symptoms\n",
        "example_query = \"I have been experiencing severe headaches, high fever, and muscle aches for the past 3 days.\"\n",
        "\n",
        "# Retrieve top 5 matches\n",
        "print(\"Searching for top 5 matches...\\n\")\n",
        "results = retrieve_top_k(example_query, k=5)\n",
        "\n",
        "# Display results\n",
        "display_results(example_query, results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Additional Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Respiratory symptoms\n",
        "query_2 = \"I have a persistent cough, shortness of breath, and chest pain.\"\n",
        "results_2 = retrieve_top_k(query_2, k=5)\n",
        "display_results(query_2, results_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: Digestive symptoms\n",
        "query_3 = \"I'm experiencing stomach pain, nausea, and diarrhea.\"\n",
        "results_3 = retrieve_top_k(query_3, k=5)\n",
        "display_results(query_3, results_3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Performance Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Measure query latency\n",
        "test_query = \"I feel dizzy and have blurred vision.\"\n",
        "num_trials = 100\n",
        "\n",
        "print(f\"Running {num_trials} queries to measure average latency...\")\n",
        "start_time = time.time()\n",
        "for _ in range(num_trials):\n",
        "    _ = retrieve_top_k(test_query, k=5)\n",
        "end_time = time.time()\n",
        "\n",
        "avg_latency = (end_time - start_time) / num_trials\n",
        "print(f\"\\nAverage query latency: {avg_latency*1000:.2f} ms\")\n",
        "print(f\"Queries per second: {1/avg_latency:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
